# -*- coding: utf-8 -*-
"""Loan_Default_Prediction.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1-ffpUcDyuBKSLG-i8VO5wQqZ7K43kJXJ
"""

from google.colab import drive
drive.mount('/content/gdrive')

import pandas as pd

df = pd.read_csv("/content/gdrive/My Drive/Loan_Default_Project/TrainingData.csv")

df_new = df.apply(pd.to_numeric, errors = 'coerce')

df_new = df_new.drop(['mvar47'], axis=1)
df_C = pd.get_dummies(df['mvar47'])
df_new['mvar47_new'] = df_C['C']

# train_cols = list(df_new)# Use MICE to fill in each row's missing features
# train = pd.DataFrame(MICE(verbose=False).fit_transform(df_new))
# train.columns = train_cols
import operator
import numpy as np

t = df_new[['mvar48','default_ind']]
tvalues = t['mvar48'].value_counts().keys().tolist()
tcounts = t['mvar48'].value_counts().tolist()
tdictionary = dict(zip(tvalues, tcounts))
s = t.where(t['default_ind'] == 1)
values = s['mvar48'].value_counts().keys().tolist()
counts = s['mvar48'].value_counts().tolist()
dictionary = dict(zip(values, counts))
per = []
val = []
for i in tdictionary:
   if(i in dictionary):
       den = dictionary[i]
       per.append((den/tdictionary[i])*100)
       val.append(i)
   else:
       den = 0
       per.append((den/tdictionary[i])*100)
       val.append(i)
sort_axis = operator.itemgetter(1)
sorted_zip = sorted(zip(val,per), key=sort_axis)
val,per = zip(*sorted_zip)
lin = np.linspace(1,67,67)
dic = dict(zip(val,(lin+300)))
df_new.replace({'mvar48':dic},inplace=True)

df_new[['mvar46','mvar5','mvar18','mvar19','mvar16','mvar30']].fillna(0,inplace=True)
df_new['mvar28'].fillna(3,inplace=True)
df_new['mvar43'].fillna(5,inplace=True)
df_new[['mvar32','mvar33']].fillna(df_new.median(),inplace=True)
df_new['mvar38'].fillna(df_new['mvar38'].mode()[0],inplace=True)
df_new.fillna(df_new.mean(),inplace=True)

df_new['default_ind'].value_counts()

y = df_new['default_ind']
target = df_new.drop(['application_key','default_ind'], axis = 1)

target_new = df_new.drop(['application_key','mvar3','mvar4','mvar8','mvar9','mvar17','mvar20','mvar22','mvar24','mvar26','mvar27','mvar29','mvar31','mvar35','mvar36','mvar45','default_ind'], axis = 1)

from sklearn.linear_model import LogisticRegression
from xgboost import XGBClassifier
from sklearn.ensemble import RandomForestClassifier
from sklearn.model_selection import cross_val_score, GridSearchCV
import operator
# clf = LogisticRegression()
# scores = cross_val_score(clf, target, y, cv=5)
from sklearn import preprocessing
from sklearn.metrics import f1_score, make_scorer
from sklearn.neighbors import KNeighborsClassifier

target_std = preprocessing.scale(target)

# target_new = target.drop(['mvar3','mvar4','mvar8','mvar9','mvar17','mvar20','mvar22','mvar24','mvar26','mvar27','mvar29','mvar31','mvar35','mvar36','mvar45'], axis = 1)
clf = KNeighborsClassifier(5)
scores = cross_val_score(clf, target_new, y,cv=5)
scores

df_new['sum_14_15'] = df_new['mvar14'] + df_new['mvar15']
df_new['12/sum_1415'] = df_new['mvar12']/df_new['sum_14_15']
df_new['13/sum_1415'] = df_new['mvar13']/df_new['sum_14_15']
df_new['9/10'] = df_new['mvar9']/df_new['mvar10']
df_new['11/12'] = df_new['mvar11']/df_new['mvar12']
df_new['22/23'] = df_new['mvar22']/df_new['mvar23']
df_new['30/31'] = df_new['mvar30']/df_new['mvar31']
df_new['28/29'] = df_new['mvar28']/df_new['mvar29']

df_new = df_new.replace([np.inf, -np.inf], np.nan)
df_new = df_new.dropna()

target_new = df_new.drop(['application_key','mvar3','mvar4','sum_14_15','mvar9','mvar10','mvar11','mvar12','mvar13', 'mvar14','mvar15','mvar17','mvar20','mvar22','mvar23','mvar24','mvar28','mvar29','mvar30','mvar31','mvar35','mvar36','mvar45','default_ind'], axis = 1)
y = df_new['default_ind']

target_new.dtypes

clf = XGBClassifier()
scores = cross_val_score(clf, target_new, y,cv=5)
scores

df_test = pd.read_csv("/content/gdrive/My Drive/testX.csv")

df_new = df_test.apply(pd.to_numeric, errors = 'coerce')

df_new = df_new.drop(['mvar47'], axis=1)
df_C = pd.get_dummies(df_test['mvar47'])
df_new['mvar47_new'] = df_C['C']

df_new.head()

df_new.replace({'mvar48':dic},inplace=True)

df_new[['mvar46','mvar5','mvar18','mvar19','mvar16','mvar30']].fillna(0,inplace=True)
df_new['mvar28'].fillna(3,inplace=True)
df_new['mvar43'].fillna(5,inplace=True)
df_new[['mvar32','mvar33']].fillna(df_new.median(),inplace=True)
df_new['mvar38'].fillna(df_new['mvar38'].mode()[0],inplace=True)
df_new.fillna(df_new.mean(),inplace=True)

clf = XGBClassifier()
clf.fit(target_new, y)

# test_new = df_new.drop(['application_key','mvar3','mvar4','mvar8','mvar9','mvar12','mvar13', 'mvar14','mvar15','mvar17','mvar20','mvar22','mvar24','mvar26','mvar27','mvar29','mvar31','mvar35','mvar36','mvar45'], axis = 1)

df_new['sum_14_15'] = df_new['mvar14'] + df_new['mvar15']
df_new['12/sum_1415'] = df_new['mvar12']/df_new['sum_14_15']
df_new['13/sum_1415'] = df_new['mvar13']/df_new['sum_14_15']
df_new['9/10'] = df_new['mvar9']/df_new['mvar10']
df_new['11/12'] = df_new['mvar11']/df_new['mvar12']
df_new['22/23'] = df_new['mvar22']/df_new['mvar23']
df_new['30/31'] = df_new['mvar30']/df_new['mvar31']
df_new['28/29'] = df_new['mvar28']/df_new['mvar29']

test_new = df_new.drop(['application_key','mvar3','mvar4','mvar9','sum_14_15','mvar10','mvar11','mvar12','mvar13', 'mvar14','mvar15','mvar17','mvar20','mvar22','mvar23','mvar24','mvar28','mvar29','mvar30','mvar31','mvar35','mvar36','mvar45'], axis = 1)

test_new.dtypes

y_pred = clf.predict(test_new)

y_pred

arr = df_new['application_key']
dataset = pd.DataFrame({'application_key': arr, 'default_ind': y_pred})
dataset.to_csv('/content/gdrive/My Drive/submission_5.csv', header = False, index= False)

y_pred

from sklearn.decomposition import PCA

pca = PCA(n_components=40)

data_new = pca.fit_transform(target)

data_new.shape

df_new.shape

df_new = df_new.drop(['application_key','default_ind'], axis=1)

pca.explained_variance_ratio_

from sklearn.model_selection import train_test_split

X_train, X_test, y_train, y_test = train_test_split(
    data_new, y, test_size=0.25, random_state=42)
clf = XGBClassifier()
clf.fit(X_train, y_train)
clf.score(X_test,y_test)
y_pred = clf.predict(X_test)

y = df['default_ind']

f1_score(y_pred, y_test)

test_data = df_new.drop(['application_key'], axis =1)

test_data.shape

test_pca = pca.fit_transform(test_data)

clf.fit(data_new, y)

y_pred = clf.predict(test_pca)

y_pred

arr = df_test['application_key']
dataset = pd.DataFrame({'application_key': arr, 'default_ind': y_pred})
dataset.to_csv('/content/gdrive/My Drive/submission_3.csv', header = False, index= False)

from sklearn.linear_model import LassoCV
y = df['default_ind']
model =  LassoCV(cv=5, random_state=0).fit(target, y)

model.coef_

target.dtypes

def create_baseline():
# create model
  model = Sequential()
  model.add(Dense(32, input_dim=35, activation='relu'))
  model.add(Dense(1, activation='sigmoid'))
  # Compile model
  model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])
  return model
# evaluate baseline model with standardized dataset
estimators = []
estimators.append(('standardize', StandardScaler()))
estimators.append(('mlp', KerasClassifier(build_fn=create_baseline, epochs=100, batch_size=5, verbose=0)))
pipeline = Pipeline(estimators)
kfold = StratifiedKFold(n_splits=10, shuffle=True)
results = cross_val_score(pipeline, target_new.values, y.values, cv=kfold)
print("Standardized: %.2f%% (%.2f%%)" % (results.mean()*100, results.std()*100))

from sklearn.preprocessing import StandardScaler

from keras.models import Sequential
from keras.layers import Dense, Activation
from keras.wrappers.scikit_learn import KerasClassifier
from sklearn.model_selection import StratifiedKFold
from sklearn.model_selection import cross_val_score
import numpy
from sklearn.pipeline import Pipeline

np.all(np.isfinite(target_new.values))

target_std = StandardScaler(target_new)

test_new.isnull().sum()

test_new.fillna(0,inplace=True)

test_new.replace([np.inf, -np.inf], np.nan, inplace= True)

param_test1 = {
 'max_depth':range(3,10,2),
 'min_child_weight':range(1,6,2)
}
gsearch1 = GridSearchCV(estimator = XGBClassifier( learning_rate =0.1, n_estimators=140, max_depth=5,
 min_child_weight=1, gamma=0, subsample=0.8, colsample_bytree=0.8,
 objective= 'binary:logistic', nthread=4, scale_pos_weight=1, seed=27),
 param_grid = param_test1, scoring='roc_auc',n_jobs=4,iid=False, cv=5)
gsearch1.fit(target_new,y)
gsearch1.grid_scores_, gsearch1.best_params_, gsearch1.best_score_

df_new

